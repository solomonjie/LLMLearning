{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7523427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch GPU诊断 ===\n",
      "PyTorch版本: 2.8.0+cu129\n",
      "CUDA是否可用: True\n",
      "CUDA版本: 12.9\n",
      "\n",
      "=== 构建信息 ===\n",
      "使用CUDA构建: True\n",
      "cuDNN可用: True\n",
      "cuDNN版本: 91002\n",
      "\n",
      "=== 包详细信息 ===\n",
      "\n",
      "=== CUDA运行时测试 ===\n",
      "GPU数量: 1\n",
      "GPU 0: NVIDIA GeForce GTX 1650\n",
      "\n",
      "=== 系统检查 ===\n",
      "nvidia-smi执行成功\n",
      "  Sun Dec  7 17:55:54 2025       \n",
      "  +-----------------------------------------------------------------------------------------+\n",
      "  | NVIDIA-SMI 581.57                 Driver Version: 581.57         CUDA Version: 13.0     |\n",
      "  +-----------------------------------------+------------------------+----------------------+\n",
      "  | GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "  | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "  |                                         |                        |               MIG M. |\n",
      "  |=========================================+========================+======================|\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def detailed_diagnostic():\n",
    "    print(\"=== PyTorch GPU诊断 ===\")\n",
    "    \n",
    "    # 1. PyTorch基本信息\n",
    "    print(f\"PyTorch版本: {torch.__version__}\")\n",
    "    print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA版本: {getattr(torch.version, 'cuda', 'None')}\")\n",
    "    \n",
    "    # 2. 构建信息\n",
    "    print(f\"\\n=== 构建信息 ===\")\n",
    "    print(f\"使用CUDA构建: {torch.backends.cuda.is_built()}\")\n",
    "    print(f\"cuDNN可用: {torch.backends.cudnn.is_available()}\")\n",
    "    print(f\"cuDNN版本: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}\")\n",
    "    \n",
    "    # 3. 检查是否是CPU版本的PyTorch\n",
    "    print(f\"\\n=== 包详细信息 ===\")\n",
    "    try:\n",
    "        import pip\n",
    "        packages = pip.get_installed_distributions()\n",
    "        torch_pkg = [p for p in packages if 'torch' in p.key][0]\n",
    "        print(f\"Torch包名称: {torch_pkg}\")\n",
    "        print(f\"Torch包位置: {torch_pkg.location}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 4. 尝试直接与CUDA运行时交互\n",
    "    print(f\"\\n=== CUDA运行时测试 ===\")\n",
    "    if hasattr(torch.cuda, 'is_available') and torch.cuda.is_available():\n",
    "        print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"PyTorch报告CUDA不可用\")\n",
    "        \n",
    "    # 5. 检查系统环境\n",
    "    print(f\"\\n=== 系统检查 ===\")\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"nvidia-smi执行成功\")\n",
    "            # 提取关键信息\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if i < 8:  # 只显示前8行关键信息\n",
    "                    print(f\"  {line}\")\n",
    "        else:\n",
    "            print(\"nvidia-smi执行失败\")\n",
    "    except Exception as e:\n",
    "        print(f\"nvidia-smi错误: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detailed_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6264bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl # original dataset for fine-tuning\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl # part of fine-tuning dataset refined by llama-3.2-1b-instruct\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl # gsm8k public test dataset\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl # gsm8k private test dataset\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv # ailuminate test dataset (public + private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets trl bitsandbytes transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbdf5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_Token')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbd70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
    "    AutoTokenizer, # imports the tokenizer for the model\n",
    "    BitsAndBytesConfig, # imports the configuration for using bitsandbytes\n",
    "    pipeline # imports the pipeline for text generation\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, # imports the configuration for LoRA\n",
    "    get_peft_model, # imports the function to get the PEFT model\n",
    "    PeftModel # imports the PEFT model\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Sets the CUDA device to use\n",
    "device = torch.device('cuda:0') # Creates a CUDA device object\n",
    "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
    "from trl import SFTConfig, SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
    "import random\n",
    "random.seed(42) # Sets the random seed for reproducibility\n",
    "from tqdm import tqdm # Imports the tqdm library for progress bars\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df60b4",
   "metadata": {},
   "source": [
    "## LLM Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd261ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "sft_model_name = 'meta-llama/Llama-3.2-1B-Instruct' # Specifies the name of the pre-trained model to use\n",
    "sft_bnb_config = BitsAndBytesConfig( # Configuration for using bitsandbytes\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    "    quantization_config=sft_bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained( # Loads the tokenizer for the model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    ")\n",
    "sft_tokenizer.model_max_length = 10000\n",
    "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Adds a special token for padding\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # TODO: Adds dropout\n",
    "    lora_dropout=0.00,  # lora_dropout = 0 equals no dropout\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(sft_model, peft_config).to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd219f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonlines(file_name: str):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:  # 指定编码\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
    "    if mode not in ['train', 'test']:\n",
    "        raise AssertionError('Undefined Mode!!!')\n",
    "\n",
    "    chats = []\n",
    "    # TODO: Use fixed few-shot examples\n",
    "    for qna in random.sample(nshot_data, n): # Samples n examples from the n-shot data\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
    "            }\n",
    "        )\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    chats.append(\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
    "        }\n",
    "    )\n",
    "    if mode == 'train':\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chats # Returns the list of chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76ebd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14776\\2033641149.py:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  gsm8k_train = load_jsonlines('HW8Dataset\\gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n"
     ]
    }
   ],
   "source": [
    "gsm8k_train = load_jsonlines('HW8Dataset\\gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "\n",
    "formatted_gsm8k = []\n",
    "TRAIN_N_SHOT = 1 # TODO: Give model more examples\n",
    "for qna in gsm8k_train: # Iterates over the GSM8K training data\n",
    "    chats = nshot_chats(nshot_data=gsm8k_train, n=TRAIN_N_SHOT, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
    "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
    "    train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):] # Remove Cutting Knowledge Date in prompt template\n",
    "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
    "        {\n",
    "            'text': train_sample # Adds the text of the example\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6f57e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64acf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|start_header_id|>user<|end_header_id|>\\n\\nQ: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nA: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\\nSo, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\\nThere are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\\nSo, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\\nTherefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\\n#### 12<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQ: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step. At the end, you MUST write the answer as an integer after '####'.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nA: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72<|eot_id|>\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_gsm8k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeba647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted_gsm8k filtered: kept 2491/7473 longest examples using fields=('text',).\n"
     ]
    }
   ],
   "source": [
    "### Please do not modify this block ###\n",
    "# Keep the longest 1/3 of `formatted_gsm8k` by letter count\n",
    "PORTION = 1/3  # change this if needed\n",
    "\n",
    "def _letters(s):\n",
    "    s = \"\" if s is None else (s if isinstance(s, str) else str(s))\n",
    "    return sum(1 for ch in s if ch.isalpha())\n",
    "\n",
    "# Choose fields: prefer 'text' if present, else fall back to ('question','answer')\n",
    "cols = getattr(formatted_gsm8k, \"column_names\", None) or []\n",
    "FIELDS = (\"text\",) if \"text\" in cols else (\"question\", \"answer\")\n",
    "\n",
    "n = len(formatted_gsm8k)\n",
    "k = max(1, int(round(n * PORTION)))\n",
    "\n",
    "# Compute lengths and take top-k indices\n",
    "lengths = []\n",
    "for i in range(n):\n",
    "    ex = formatted_gsm8k[i]  # dict-like\n",
    "    lengths.append(sum(_letters(ex.get(f, \"\")) for f in FIELDS))\n",
    "\n",
    "top_idx = sorted(range(n), key=lambda i: lengths[i], reverse=False)[:k] #modified to shortest 1/3\n",
    "formatted_gsm8k = formatted_gsm8k.select(top_idx)\n",
    "\n",
    "print(f\"formatted_gsm8k filtered: kept {k}/{n} longest examples using fields={FIELDS}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccde446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|██████████| 2491/2491 [00:00<00:00, 12417.31 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2491/2491 [00:01<00:00, 1268.68 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2491/2491 [00:00<00:00, 235072.03 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128256}.\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='623' max='623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [623/623 2:16:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=623, training_loss=0.8426518631593757, metrics={'train_runtime': 8203.826, 'train_samples_per_second': 0.304, 'train_steps_per_second': 0.076, 'total_flos': 4257033117769728.0, 'train_loss': 0.8426518631593757, 'entropy': 0.822012939527965, 'num_tokens': 724886.0, 'mean_token_accuracy': 0.7986090659026073, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer\n",
    "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
    "    seed=1126,\n",
    "    data_seed=1126,\n",
    "    output_dir=f\"sft\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=1e-4, \n",
    "\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    dataset_text_field='text',\n",
    "    report_to='none',\n",
    ")\n",
    "trainer = SFTTrainer( # Creates the SFT trainer\n",
    "    model=peft_model,\n",
    "    train_dataset=formatted_gsm8k,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=sft_tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "trainer.train() # Starts the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043103a",
   "metadata": {},
   "source": [
    "## LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline( # Creates a text generation pipeline\n",
    "    'text-generation',\n",
    "    model=sft_model,\n",
    "    tokenizer=sft_tokenizer,\n",
    "    pad_token_id=sft_tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, # TODO: Increase max_new_tokens for longer output\n",
    "    # TODO: Use greedy decoding strategy\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "adapter_path = 'sft/checkpoint-567' # TODO: Evaluate different checkpoints (check the actuall checkpoint step from \"檔案\")\n",
    "pipeline.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
    "    sft_model,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.bfloat16, ##Added for A100/L4\n",
    ")\n",
    "pipeline.model.to(dtype=torch.bfloat16, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c7d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(chats: list): # Function to get the response from the model\n",
    "    gen_text = generator(chats)[0]  # First return sequence\n",
    "    return gen_text['generated_text'][-1]['content'] # Returns the content of the last generated text\n",
    "\n",
    "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
    "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
    "\n",
    "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    return answer # Returns the extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c35f420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\g'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14776\\2107852672.py:4: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  gsm8k_test_public = load_jsonlines('HW8Dataset\\gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14776\\2107852672.py:29: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  gsm8k_test_private = load_jsonlines('HW8Dataset\\gsm8k_test_private.jsonl') # Loads the GSM8K private test data\n",
      "GSM8K Public Test Data Evaluation:  10%|█         | 10/100 [01:09<10:35,  7.06s/it, Current Accuracy = 0.400]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "GSM8K Public Test Data Evaluation: 100%|██████████| 100/100 [16:32<00:00,  9.93s/it, Current Accuracy = 0.350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K Public Test Data Evaluation Complete, Total Accuracy: 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GSM8K Private Test Data Inference: 100%|██████████| 100/100 [16:14<00:00,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K Private Test Data Inference Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gsm8k_predictions = []\n",
    "TEST_N_SHOT = 1 # TODO: give model more examples\n",
    "\n",
    "gsm8k_test_public = load_jsonlines('HW8Dataset\\gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
    "gsm8k_test_public = gsm8k_test_public[0:100] # We use only 100 of the original 13\n",
    "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
    "\n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
    "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
    "        correct += 1 # Increments the correct count if the prediction is correct\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/gsm8k_total:.3f}') # Prints the total accuracy on the public test data\n",
    "\n",
    "gsm8k_test_private = load_jsonlines('HW8Dataset\\gsm8k_test_private.jsonl') # Loads the GSM8K private test data\n",
    "gsm8k_test_private = gsm8k_test_private[0:100]\n",
    "gsm8k_total = len(gsm8k_test_private) # Gets the total number of examples in the private test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Private Test Data Inference') # Creates a progress bar for the private test data evaluation\n",
    "\n",
    "for i, qna in enumerate(gsm8k_test_private): # Iterates over the private test data\n",
    "\n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Private Test Data Inference Complete') # Prints a message indicating that the private test data evaluation is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bcc18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_name: str):\n",
    "    csvfile = open(file_name,'r', encoding='utf-8')\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    questions = []\n",
    "    for row in rows:\n",
    "        questions.append(row['prompt_text'])\n",
    "    return questions\n",
    "\n",
    "# def load_jsonlines(file_name: str):\n",
    "#     with open(file_name, 'r', encoding='utf-8') as f:  # 指定编码\n",
    "#         return [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028db803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AILuminate Test Data Evaluation:  50%|█████     | 40/80 [11:35<12:07, 18.18s/it]"
     ]
    }
   ],
   "source": [
    "ailuminate_predictions = []\n",
    "\n",
    "ailuminate_test = load_csv('HW8Dataset\\\\ailuminate_test.csv') # Loads the AILuminate test data\n",
    "ailuminate_public = ailuminate_test[0:40]\n",
    "ailuminate_private = ailuminate_test[120:160]\n",
    "ailuminate_test = ailuminate_public + ailuminate_private\n",
    "ailuminate_total = len(ailuminate_test) # Gets the total number of examples in the AILuminate test data\n",
    "ailuminate_progress_bar = tqdm(total=ailuminate_total, desc='AILuminate Test Data Evaluation') # Creates a progress bar for the AILuminate test data evaluation\n",
    "\n",
    "for i, question in enumerate(ailuminate_test): # Iterates over the AILuminate test data\n",
    "\n",
    "    message = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }\n",
    "    ]\n",
    "    response = get_response(message) # Gets the response from the model\n",
    "    ailuminate_predictions.append(response) # Appends the response to the list of predictions\n",
    "\n",
    "    ailuminate_progress_bar.update() # Updates the progress bar\n",
    "ailuminate_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'AIluminate Test Data Evaluation Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

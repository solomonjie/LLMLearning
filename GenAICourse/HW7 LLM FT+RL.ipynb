{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e27120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 18 21:02:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.57                 Driver Version: 581.57         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   54C    P8              3W /   50W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dd455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -q --upgrade torch torchvision torchaudio\n",
    "%pip install -q --upgrade \"transformers[torch]\"\n",
    "%pip install -q --upgrade accelerate datasets\n",
    "%pip install -q bitsandbytes trl[peft] loralib huggingface_hub\n",
    "%pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78455917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4a1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf2c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_Token')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3386aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_generation_prompt(tokenizer):\n",
    "    generation_chat_template = \"\"\"{{ bos_token }}\n",
    "{%- if messages[0]['role'] == 'system' -%}\n",
    "    {%- if messages[0]['content'] is string -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\n",
    "    {%- else -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\n",
    "    {%- endif -%}\n",
    "    {%- set loop_messages = messages[1:] -%}\n",
    "{%- else -%}\n",
    "    {%- set first_user_prefix = \"\" -%}\n",
    "    {%- set loop_messages = messages -%}\n",
    "{%- endif -%}\n",
    "{%- for message in loop_messages -%}\n",
    "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
    "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
    "    {%- endif -%}\n",
    "    {%- if (message['role'] == 'assistant') -%}\n",
    "        {%- set role = \"model\" -%}\n",
    "    {%- else -%}\n",
    "        {%- set role = message['role'] -%}\n",
    "    {%- endif -%}\n",
    "    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }}\n",
    "    {%- if message['role'] == 'assistant' -%}\n",
    "        {% generation %}\n",
    "        {%- if message['content'] is string -%}\n",
    "            {{ message['content'] | trim }}\n",
    "        {%- elif message['content'] is iterable -%}\n",
    "            {%- for item in message['content'] -%}\n",
    "                {%- if item['type'] == 'image' -%}\n",
    "                    {{ '<start_of_image>' }}\n",
    "                {%- elif item['type'] == 'text' -%}\n",
    "                    {{ item['text'] | trim }}\n",
    "                {%- endif -%}\n",
    "            {%- endfor -%}\n",
    "        {%- else -%}\n",
    "            {{ raise_exception(\"Invalid content type\") }}\n",
    "        {%- endif -%}\n",
    "        {{ '<end_of_turn>\\n' }}\n",
    "        {% endgeneration %}\n",
    "    {%- else -%}\n",
    "        {%- if message['content'] is string -%}\n",
    "            {{ message['content'] | trim }}\n",
    "        {%- elif message['content'] is iterable -%}\n",
    "            {%- for item in message['content'] -%}\n",
    "                {%- if item['type'] == 'image' -%}\n",
    "                    {{ '<start_of_image>' }}\n",
    "                {%- elif item['type'] == 'text' -%}\n",
    "                    {{ item['text'] | trim }}\n",
    "                {%- endif -%}\n",
    "            {%- endfor -%}\n",
    "        {%- else -%}\n",
    "            {{ raise_exception(\"Invalid content type\") }}\n",
    "        {%- endif -%}\n",
    "        {{ '<end_of_turn>\\n' }}\n",
    "    {%- endif -%}\n",
    "{%- endfor -%}\n",
    "{%- if add_generation_prompt -%}\n",
    "    {{'<start_of_turn>model\n",
    "'}}\n",
    "{%- endif -%}\"\"\"\n",
    "    tokenizer.chat_template = generation_chat_template\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Define a helper function to load and set up the model and tokenizer\n",
    "def get_model_tokenizer(model_name, return_model=True, return_tokenizer=True):\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    if return_tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = add_generation_prompt(tokenizer)\n",
    "    if return_model:\n",
    "        # Set up the quantization config\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_use_double_quant=True,\n",
    "          bnb_4bit_quant_type=\"nf4\",\n",
    "          bnb_4bit_compute_dtype=\"bfloat16\"\n",
    "        )\n",
    "        # Load the model from Huggingface and apply quantization\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          quantization_config=quant_config,\n",
    "          trust_remote_code=True,\n",
    "          low_cpu_mem_usage=True,\n",
    "        )\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    if return_model and return_tokenizer:\n",
    "        tokenizer.pad_token_id = 0\n",
    "        tokenizer.eos_token_id = 1\n",
    "        model.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_adapter(model, adapter_name):\n",
    "    result_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        adapter_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6d4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FULL_MODEL = False # Set to True if you want to save the full model\n",
    "\n",
    "SAVE_TO_DRIVE = False # Set to True if you want to save the model to your Google Drive\n",
    "# Modify CKPT_PATH to the path you want to save\n",
    "CKPT_PATH = \"\\\\HW7\"\n",
    "\n",
    "# CAUTION: If both SAVE_FULL_MODEL and SAVE_TO_DRIVE is set True, ensure your Google Drive has sufficient space.\n",
    "# Otherwise it is very possible that you exceed your drive space\n",
    "\n",
    "USE_WANDB = False # Set to True if you want to use wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62e2f4",
   "metadata": {},
   "source": [
    "## Phase 1: SFT - Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcd6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cd9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [03:57<00:00, 59.28s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"jaxon3062/gemma-3-4b-pt-chat\"\n",
    "model, tokenizer = get_model_tokenizer(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941eee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\enlist\\LLMLearning\\llmlearning_env\\Lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "def chat_interface(message, history):\n",
    "    # Format the chat history for the model\n",
    "    prompt = \"\"\n",
    "    SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "    prompt += SYSTEM_PROMPT\n",
    "    for human, assistant in history:\n",
    "        prompt += human\n",
    "        prompt += assistant\n",
    "    prompt += message\n",
    "\n",
    "    # Get the model response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"])\n",
    "        )\n",
    "        output = tokenizer.decode(out[0], skip_special_tokens=False).strip()\n",
    "        response = tokenizer.decode(out[0][len(inputs[\"input_ids\"]):], skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    title=\"Gemma 3 4b Chat\",\n",
    "    description=\"Chat with the Gemma model.\",\n",
    "    examples=[\n",
    "        [\"Where is the capital of France?\"],\n",
    "        [\"Who is Julius Caesar?\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "iface.launch(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
